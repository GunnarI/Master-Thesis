\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter{Discussion}
\label{sec:discussion}
This chapter will be divided, like the results chapter, into two sections. 
The first chapter discusses the pre-training results where the results from the experiment and the pre-processing of the data is discussed.
The second chapter discusses the performance results of the trained model.
This includes a further discussion about why the model might generalize well for some cases but bad for others.

\section{Pre-training results}
\label{sec:discussion_pre-training-results}
The \ac{EMG} pattern from most of the muscles was shown to highly correlate between subjects.
This suggests that the subjects have similar muscle activation patterns as well as \ac{KJM} during the gait cycle.
Therefore, it could be possible to train a \ac{NN} to map the \ac{EMG} signal to \ac{KJM}, and generalize across subjects. 
With that stated, it should be emphasized that the subjects were all young, healthy adults.
Many have researched difference in \ac{EMG} signal between various target groups and varying conditions \cite{Courtine2003,Rezgui2013,Sacco2010, Zwaan2012}.
From these researches, it can be concluded that \ac{EMG} signal can vary in amplitude, lag, and/or pattern depending on pathology and conditions.

The \ac{EMG} from gluteus maximus was observed to vary between subjects, much more than the \ac{EMG} from other muscles.
There could be a number of reasons for this discrepancy.
For example, differing location of the sensor.
The researcher found it more difficult to determine the correct sensor location for the gluteus maximus than the other muscles.
It should also be mentioned that the sensor was the only sensor that was located underneath the clothes (shorts) of the user.
The shorts could have got caught in the sensor during walking, possibly pulling on it.
Furthermore, underneath the clothing there could be more perspiration, which could affect the signal.
More specifically, sweat has been shown to dampen the signal \cite{Abdoli-Eramaki2012}.

\section{Post-training results}
\label{sec:discussion_post-training-results}
Add text about how the models compare
When looking at the results from section \ref{sec:post-training-results}, it is clear that the same model does not perform as well for every case.
The \ac{MLP} model generally performed much worse than the \ac{LSTM} model and did not manage to generalize well.
The \ac{MSE} of the training set always converged at a much higher value for the \ac{MLP} model than the \ac{LSTM} model.
This indicates that an \ac{MLP} cannot be used to generalize between sessions.
The \ac{MLP} is only looking at \ac{EMG} data at one timestep, and trying to map it to a \ac{KJM} at the same timestep.
Arguably, this does not capture the task specific pattern of the \ac{EMG} as well as the \ac{LSTM} model.

The design of the model had some iterations. 
One notable design criteria, that changed throughout the process, is the number of timesteps included in the \ac{LSTM} model.
Looking at 20 past samples of \ac{EMG} results in a rather deep network when considering running it on an embedded system.
At first, much fewer timesteps were used.
Those models performed alright in cases where the testing set was a part of the same session's dataset as the training set.
However, they completely failed to generalize between subjects.
It was not until using 20 timesteps that the model was able to generalize to some extent.

\subsection{Case 1}
\label{sec:discussion-case1}
Case 1 is where the model was trained on one session of subject 6 and tested on another session of same subject.
This case was intended for looking at the ability of the model to generalize across sessions.
In each session the \ac{EMG} sensors were reapplied as per the \ac{SENIAM} guidelines \cite{Hermens1999, Hermens2000}.
However, the location and orientation of the sensors are most likely not exactly the same between sessions.
Furthermore, the condition of the subjects is also not the same between session.
Number of factors may affect difference between the sessions such as muscle soreness or tiredness, more sweating by the subject, or other skin conditions.

Subject 6 was used here as it was the only subject with large enough datasets from separate sessions to reliably train and test the model.
The model was trained on the bigger dataset, i.e. session 1.
The result showed little overfitting, with the \ac{MSE} of the validation set and testing set not being much higher than for the training set.
The error was generally rather low and the model seemed to perform well on the test set.

\subsection{Case 2}
\label{sec:discussion-case2}
In case 2, both sessions from subjects 1 and 2 were added to the training set.
This was done to look at how increasing variance in the training set would affect the results from case 1.

It is interesting to see that the training error and validation error was significantly decreased and the discrepancy between them was low, indicating decreased risk of overfitting.
However, the error of the test set was increased from case 1.
This is perhaps not surprising, since the difference in \ac{EMG} signals can be assumed to be more between subjects rather than between sessions of the same subject.
The increase in error is however, not large, and the model still performs reasonably well.
Further research could add additional sessions of the same subject to test if this case increases flexibility to higher variation between sessions.
More specifically, to test if the overall testing error of several separate sessions would decrease when using case 2 rather than case 1.

\subsection{Case 3}
\label{sec:discussion-case3}
For case 3 the model failed to generalize well.
The results indicate high level of overfitting with the error of the training set being much lower than for the validation set and the testing set.

\subsection{Case 4}
\label{sec:discussion-case4}

\subsection{Case 5}
\label{sec:discussion-case5}

% The study included 5 subjects and two of those subjects measured minimal amount of non-corrupt data.
% Thus, only 3 subjects measured enough data to be used to train the \acp{NN}.
% From this small population it is difficult to draw any meaningful conclusion.

\section{Real-time compatability}
\label{sec:discussion_real-time}
When used in an exoskeleton, the assistance (or resistance) of the exoskeleton may affect the \ac{EMG} signal.
Since the model is based on the 20 past \ac{EMG} signals, this may create a bad feed-back loop.
For example, if the model uses \ac{EMG} signals from $t-20$ to $t-1$ to make the exoskeleton assist knee-flexion at $t$ then this could affect the \ac{EMG} signal collected at $t$.
This \ac{EMG} signal may be contradicting intended movement, and thus the prediction at $t+1$ might not be correct.
This in turn has more affect on the user's motion and their \ac{EMG} signal, thus resulting in the bad feed-back loop.

\end{document}