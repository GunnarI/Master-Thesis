\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter{Methods}
\label{sec:methods}
The process from data collection to training to then using the \ac{NN} is presented in figure \ref{fig:process-diagram}. 
The process can be divided into 5 steps (A-E), each of which will be discussed in dedicated subsections in this chapter.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{img/ProcessDiagram}
    \caption{Flow diagram of the modelling procedure. The dashed box indicates a procedure used only for training the \ac{NN} and the dashed arrows are feedback during training. (A) Subject walks in a motion lab collecting \acf{GRF}, marker trajectories, and muscle activity; (B) The data is filtered and prepared for use in the models; (C) The \ac{GRF} and marker trajectories are used for \ac{IK} and \ac{ID} to retrieve \acp{KJM}; (D) During training, \ac{EMG} data is continuously fed through the \ac{NN}. Output is compared to the moments from (C) and the error used as feedback to update the \ac{NN}; (E) \ac{EMG} fed through the trained \ac{NN} produces \acp{KJM} prediction to control motor torque in an exoskeleton}
    \label{fig:process-diagram}
\end{figure}

\section{Step A - Experiment Setup}
For the experiment, a motion lab equipped with 10 Vicon Vantage V16 cameras and 3 AMTI OR6 \acp{FP} was used. 
Figure \ref{fig:cgm23-markerset} shows the markerset protocol used, which follows the CGM2.3 model's protocol. 
CGM2.3 is a recent version of CGM2.i project which is meant to update the old \ac{CGM} by improving it and fixing some known issues \cite{Leboeuf2019}. 
Further details about the model and its scripts are discussed in appendix \ref{sec:A-pyCGM2}.
\begin{figure}[!ht]
     \centering
     \begin{subfigure}[t]{0.58\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/CGM23_markerset.pdf}
         \caption{Adapted from pyCGM2 official website \cite{Leboeuf2019}}
         \label{fig:cgm23-markerset-guidelines}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.40\textwidth}
         \centering
         \raisebox{10mm}{\includegraphics[width=\textwidth]{img/experiment_capture2.pdf}}
         \caption{Screenshot from Vicon Nexus software during gait trial.}
         \label{fig:cgm23-markerset-experiment-capture}
     \end{subfigure}
    \caption{CGM2.3 marker set as seen from (a) the guidelines and (b) one conducted gait trial. (a) The RBAK optional marker was included and calibration only markers were removed before dynamic trials. (b) The colored spheres represent the markers placed on the subject.}
    \label{fig:cgm23-markerset}
\end{figure}

The aktos nano \ac{sEMG} sensors from myon were used for the \ac{EMG} data capture. 
The sensors were placed on the surface near 12 muscles on the right leg, presented in table \ref{tab:muscle-names}. 
\input{tables/muscle-names.tex}
Skin preparation and placement recommendations from the SENIAM project \cite{Stegeman2007, Hermens1999, Hermens2000} were used. The SENIAM project is a European concerted action in the Biomed 2 program of the European Community \cite{Stegeman2007}. It provides recommendations when measuring \ac{sEMG} to increase standardization within the field.

All the data from the \ac{EMG} sensors (muscle activity), cameras (marker trajectories), and the \acp{FP} (\ac{GRF}) was collected and synchronized using the Vicon Nexus 2.8.2 system/software.
The synchronization includes accounting for the latency of $14ms$ of the \ac{EMG} sensors \cite{aktosEMG}.
The marker trajectories were sampled at 100Hz using the cameras while the analog data (i.e. \ac{GRF} and \ac{EMG}) was sampled at 1000Hz.

Data was collected from 6 subjects with anthropometric data measured for each subject, presented in \ref{tab:subject-table}.
\input{tables/subject-table.tex}
Each subject was a young, healthy individual without any muscle disorders.
A static trial was captured to use for scaling the model, as described in appendix \ref{sec:A-pyCGM2}, and run as part of step C in figure \ref{fig:process-diagram}.
The subjects were asked to walk front and back over the \acp{FP} either slowly, normally, or fast.
This was to increase variance in the data and thus help prevent overfitting of the \ac{NN} model.
The definition of slow, normal, or fast walking was not set to a specific speed but rather to what the subjects felt was natural.
Furthermore, about 80\% of the trials for each subject were measured with normal walking speed.
The slow and fast walking speeds each covered about 10\% of the trials for each subject.

Note that a subject refers to one individual, a session refers to each experiment performed by the subject, and a trial refers to each measured exercised performed in the session. 
Thus, each subject can have many sessions, e.g. experiments conducted on separate days.
Furthermore, each session has many trials and each trial was cut down to gait cycles. 
The gait cycles are defined by the interval from the right foot's heel strike on a \ac{FP} until its next heel strike.
Some sessions included 3 \acp{FP} resulting in some trials having two gait cycles for the right leg.

Defective cycles were found by looking at outliers, e.g. the longest/shortest cycles indicating defect in heel strike detection and the cycles with highest deviation of \ac{KJM} estimation from the mean.
Faulty signal from motion capture or \ac{GRF} makes the \ac{KJM} output from the \ac{ID} calculations wrong.
All such cycles, that were found, were removed from the dataset since the \ac{KJM} are the labels and not the inputs for the \ac{NN}.
The \ac{EMG} on the other hand is input to the model and any errors to the \ac{EMG} measurements might also happen in real live, given that it is not easily preventable.
For that reason, the cycles that contained what appeared to be random errors (i.e. not systematic) were not removed from the dataset

\section{Step B - Data processing}
\label{sec:data-processing}
In step B of figure \ref{fig:process-diagram}, before running \ac{IK} and \ac{ID}, the marker trajectories and the \ac{GRF} were filtered.
Both were filtered with a 4th order Butterworth lowpass filter and a cutoff frequency of $10Hz$. 
The same cutoff frequency was used for both to minimize an artifact introduced in the joint moment at heel strike, due to a spike in reaction force \cite{Kristianslund2012}.

The \ac{EMG} was filtered with the ``typical'' steps as mentioned by \textcite[99]{Clancy2016}, but excluding the whitening step for simplicity.
The purpose of whitening is to get rid of correlation between successive samples which affects information extraction due to temporal weights on consecutive samples.
The effect of omitting this step or the possible gain in including it will be discussed in section \ref{sec:discussion}.
The signal was noise and interference filtered using Butterworth IIR bandpass filter with cutoff frequencies $10-100Hz$. 
Then, for demodulation, smoothing, and relinearization, the RMS method was used where the smoothing was done with a moving average window of $50ms$. 
As \textcite{Clancy2016} point out, it is common to use a window of $100-250 ms$ but for the purpose of a real-time friendly solution, $50ms$ was chosen.
That is, to not introduce too much delay of the signal while still keeping it relatively smooth.

\section{Step C - Training preparation}
The training preparation consisted of creating the training, validation, and testing datasets as well as the training, validation, and testing labels.
In this case the datasets consist of the \ac{EMG} data and the labels of the corresponding \acp{KJM}.

Before dividing the data into separate datasets, the \acp{KJM} need to be estimated from the motion and \ac{GRF} data.
For that purpose, the pyCGM2 package by \textcite{Leboeuf2019} was used to apply \ac{IK} and \ac{ID} to the data.
The skeleton model is first scaled using the static trial and the Newington-Gage model with improvements from the CGM2 project, as described in appendix \ref{sec:A-Vicon-and-PiG} and \ref{sec:A-pyCGM2}.
The model is then fitted to the measured data using \ac{IK} as described in \ref{sec:A-MSModels}. 
The objective function from equation \ref{eq:ik-obj-func} is rewritten here for clarity:
\begin{align*}
    \min(obj) = \min\biggl\{ &\sum_{i=1}^{\text{markers}}w_i \left(\Vec{x_i}^{subject}-\Vec{x_i}^{model}\right)^2 \nonumber\\ 
    &+ \sum_{j=1}^{\text{joint angles}}w_j \left( \theta_j^{subject}-\theta_j^{model}\right)^2 \biggr\}
\end{align*}
The weights ($w_i$ and $w_j$) where set equal for every marker used and the threshold for the objective function was set to $10^{-8}$.
The \ac{ID} computations are based on anthropometric segment measurements according to \textcite{Dempster1955} and the iterative Newton-Euler equations as described by \textcite{Dumas2004}.
The model outputs are designed to have the same conventions as Vicon's \ac{PiG} model, and as such were normalized to the subject's height and body mass \cite{Leboeuf2019, viconpig}.
Furthermore, the \acp{KJM} were represented as positive for knee flexion and negative for extension.

The model outputs were calculated for each frame.
That means the \acp{KJM} had a sampling frequency of $100Hz$ unlike the \ac{EMG} which was sampled at $1000Hz$.
The \ac{EMG} signal was downsampled to match the frequency of the \acp{KJM}. 
The downsampling was done by simply taking the first \ac{EMG} sample from every $10ms$ windows. 
The first sample from the window was used rather than any other to decrease the delay from \ac{EMG} to moment prediction. 
In that way an \ac{EMG} sample taken at $t$ can predict a moment value at $t+9ms$. 

The final step before dividing the data into training and testing sets was feature selection.
Section \ref{sec:A-NeuralNetworks} discusses briefly the importance of feature selection.
The first step was to look at correlations in muscle activity between different muscles using the Pearson standard correlation coefficient.
Muscles which were found to correlate highly to one another and which are not contributors to knee flexion or extension were considered redundant.
To decrease complexity of the model, the data from these muscles was removed from the datasets.

Finally, once each sample had been matched, the data was split into training, validation, and testing datasets.
Five different cases of training and testing sets were created to look at the ability of the model to generalize for variable cases:
\begin{enumerate}
    \item Training set: \textit{Subject06/Session1}\\
            Testing set: \textit{Subject06/Session2}\\
            Looking at the ability to generalize across sessions.
    \item Training set: \textit{Subject01/Both, Subject02/Both, and Subject06/Session1}\\
            Testing set: \textit{Subject06/Session2}\\
            Evaluating if additional data from different subjects improves the performance from case 1.
    \item Training set: \textit{Subject01/Both and Subject06/Both}\\
            Testing set: \textit{Subject02/Both, Subject03, and Subject05}\\
            Looking at the ability to generalize across subjects.
    \item Training set: \textit{Subject02/Both and Subject06/Both}\\
            Testing set: \textit{Subject01/Both, Subject03, and Subject05}\\
            Evaluating if this compares to case 3, i.e. how sensitive is the model to which subjects it trains on.
    \item Training set: \textit{Random 70\% of all the usable data}\\
            Testing set: \textit{Random 20\% of all the usable data} (different data than the training set)\\
            Performance indicates if the same model can be used for multiple subjects as long as they have similar parameter as the subjects that the model trained on.
            
\end{enumerate}
The split was done by randomly selecting $80\%$ of the gait cycles, from each group of slow, fast, and regular walking speeds, for the training set.
The remaining $20\%$ from each group was the testing set.
Further $10\%$ of cycles in the training set were randomly selected for the validation set.

For the purpose of standardizing the data between subjects, the datasets were normalized.
\ac{EMG} was normalized so that all values from the training dataset were between $0$ and $1$, independent of subject.
The normalization is given by equation \ref{eq:emg-normalization}:
\begin{equation}
\label{eq:emg-normalization}
    X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}
\end{equation}
% \begin{equation}
% \label{eq:emg-normalization}
%     X_{norm} = \frac{X - \mu_X}{\sigma_X}
% \end{equation}
where $X$ is the \ac{EMG} value to be normalized and $X_{min}$ and $X_{max}$ are, respectively, the minimum and maximum measured \ac{EMG} from the training dataset. 
For the validation and testing datasets, the same minimum and maximum values were used, to accurately portray new data that the \ac{NN} does not already ``know''.
The \acp{KJM} were normalized similar to the \ac{EMG} in equation \ref{eq:emg-normalization} with addition clause to set the interval:
\begin{equation}
\label{eq:moment-normalization}
    X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}\left(1 - \frac{X_{min}}{X_{max}}\right) + \frac{X_{min}}{X_{max}}
\end{equation}
where $X$ represents moment values. 
The extra clause made sure the signal was scaled down to the interval $[-1,1]$ (assuming $\left|X_{min}\right| \leq \left|X_{max}\right|$) and that flexion and extension was still represented with positive and negative values respectively.

\section{Step D - Architecture and Training of the Neural Network}
As discussed in appendix \ref{sec:A-NeuralNetworks}, the training of the \ac{NN} consists of optimizing an objective function.
In this case, it involved minimizing the error between the output of the \ac{NN} and the calculated \acp{KJM}.
This is represented by the bracket and dashed arrows in step D of figure \ref{fig:process-diagram}.

Two different kinds of networks were tested for comparison, one being the simple \ac{MLP} described in section \ref{sec:A-NeuralNetworks}, and the other one was the \ac{LSTM} based \ac{RNN} described in section \ref{sec:A-rnn}.
The main difference between the two is that the \ac{LSTM} model contains time-dependency and thus looks at multiple timesteps for each prediction, while the \ac{MLP} does not.

TensorFlow \cite{tensorflow2015-whitepaper} and the Keras API \cite{chollet2015keras} were used for constructing and training the \ac{NN}.
The Keras API provides functions for configuring the \ac{LSTM} layer and the so called \textit{dense} layer, which is the regular, densily connected, \ac{NN} layer which is used in the \ac{MLP}.
The functions offer a number of parameters for the configuration, such as number of nodes, type of activation functions, regularization function, and dropout rates \cite{chollet2015keras}.
The parameters used for each model are listed in tables x and \ref{tab:lstm-parameters}.

The topology of the \ac{LSTM} based network that was used is represented in figure \ref{fig:lstm-model-used}.
\begin{figure}[ht!]
    \centering
    \includegraphics{img/LSTM_model_used.pdf}
    \caption{Topology of the \ac{LSTM} based \ac{NN} that was used. The \ac{LSTM} cells represent the structure presented in section \ref{sec:A-rnn} and figure \ref{fig:lstm-cell}. Note, there is a recurrence of cell state between each timestep, even though it is not shown here. In this thesis $\tau = 20$ was used indicating that 20 \ac{EMG} samples from past steps were used for each \ac{KJM} prediction at time step $t$.}
    \label{fig:lstm-model-used}
\end{figure}
Note that the \ac{LSTM} cell contains regular \ac{NN} layers, as presented in section \ref{sec:A-NeuralNetworks}, with a configurable number of nodes.
\input{tables/lstm-parameters.tex}


% \section{Surface-EMG filtering and feature extraction}
% The filtering steps are \parencite[99]{Clancy2016}:
% \begin{enumerate}
%     \item Noise and interference filtering using Butterworth IIR bandpass filter with cutoff frequencies $10-100Hz$
%     \item Temporal decorrelation (or whitening) to get rid of the correlation from preceding signal of the same EMG channel.
%     \item Demodulation (this could be taking RMS?)
%     \item Smoothing: moving average with a window of $20ms$, i.e. 20 samples. As \citeauthor{Clancy2016} points out, it is common to use a window of $100-250 ms$ but for the purpose of keeping the solution real-time friendly $20ms$ is chosen to not introduce too much delay.
%     \item Relinearization
% \end{enumerate}

\section{Step E - Real-time motor control and performance}
Although this part was not implemented in this thesis the following methods are thoughts about how it could be done.

Once the network has been trained the weights can be saved for later use.
This is helpful since the model would typically be run on a different system than where it was trained.
In this case the model was trained using Python and the weights of the trained model were saved to a file.
The trained model could then be transferred to C++ by using Tensorflow C++ library to replicate the model structure and then loading the saved weights onto the replica.
This can be helpful since in an exoskeleton the trained model needs to be stored on an embedded system which often runs C++ rather than Python.
Also, Vicon offers a Datastream SDK in C++ enabling real time computation that can be used with Vicon Nexus.
This is further discussed in section \ref{sec:discussion}.



\end{document}
