\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter{Discussion}
\label{sec:discussion}
This chapter will be divided, like the results chapter, into two sections. 
The first chapter discusses the pre-training results where the results from the experiment and the pre-processing of the data is discussed.
The second chapter discusses the performance results of the trained model.
This includes a further discussion about why the model might generalize well for some cases but bad for others.

\section{Pre-training results}
\label{sec:discussion_pre-training-results}
The \ac{EMG} pattern from most of the muscles was shown to highly correlate between subjects.
This suggests that the subjects have similar muscle activation patterns as well as \ac{KJM} during the gait cycle.
Therefore, it could be possible to train a \ac{NN} to map the \ac{EMG} signal to \ac{KJM}, and generalize across subjects. 
With that stated, it should be emphasized that the subjects were all young, healthy adults.
Many have researched difference in \ac{EMG} signal between various target groups and varying conditions \cite{Courtine2003,Rezgui2013,Sacco2010, Zwaan2012}.
From these researches, it can be concluded that \ac{EMG} signal can vary in amplitude, lag, and/or pattern depending on pathology and conditions.

The \ac{EMG} from gluteus maximus was observed to vary between subjects, much more than the \ac{EMG} from other muscles.
There could be a number of reasons for this discrepancy.
For example, differing location of the sensor.
The researcher found it more difficult to determine the correct sensor location for the gluteus maximus than the other muscles.
It should also be mentioned that the sensor was the only sensor that was located underneath the clothes (shorts) of the user.
The shorts could have got caught in the sensor during walking, possibly pulling on it.
Furthermore, underneath the clothing there could be more perspiration, which could affect the signal.
More specifically, sweat has been shown to dampen the signal \cite{Abdoli-Eramaki2012}.

\section{Post-training results}
\label{sec:discussion_post-training-results}
Add text about how the models compare
When looking at the results from section \ref{sec:post-training-results}, it is clear that the same model does not perform as well for every case.
The \ac{MLP} model generally performed much worse than the \ac{LSTM} model and did not manage to generalize well.
The \ac{MSE} of the training set always converged at a much higher value for the \ac{MLP} model than the \ac{LSTM} model.
Regardless, with respect to the error on the test set, the model is observed to be overfitting.
This indicates that an \ac{MLP} cannot be used to generalize between sessions.
The \ac{MLP} is only looking at \ac{EMG} data at one timestep, and trying to map it to a \ac{KJM} at the same timestep.
Arguably, this does not capture the task specific pattern of the \ac{EMG} as well as the \ac{LSTM} model.

The design of the model had some iterations. 
One notable design criteria, that changed throughout the process, is the number of timesteps included in the \ac{LSTM} model.
Looking at 20 past samples of \ac{EMG} results in a rather deep network when considering running it on an embedded system.
At first, much fewer timesteps were used.
Those models performed alright in cases where the testing set was a part of the same session's dataset as the training set.
However, they completely failed to generalize between subjects.
It was not until using 20 timesteps that the model was able to generalize to some extent.

\subsection{Case 1}
\label{sec:discussion-case1}
Case 1 was intended for looking at the ability of the model to generalize across sessions.
In each session the \ac{EMG} sensors were reapplied as per the \ac{SENIAM} guidelines \cite{Hermens1999, Hermens2000}.
However, the location and orientation of the sensors are most likely not exactly the same between sessions.
Furthermore, the condition of the subjects is also not the same between session.
Number of factors may affect difference between the sessions such as muscle soreness or tiredness, more sweating by the subject, or other skin conditions.

The results indicated that the model can generalize well between sessions.
However, this test should be conducted on more subjects with added number of separate sessions to reach a reliable conclusion.

\subsection{Case 2}
\label{sec:discussion-case2}
The increased test set error in case 2 as compared to case 1 is perhaps not surprising. 
The difference in \ac{EMG} signals can be assumed to be more between subjects rather than between sessions of the same subject.
The increase in error is however, not large, and the model still performs reasonably well.
Further research could add more sessions from the same subject to the testing set, in order to see if there is increased flexibility to higher variation between sessions.
More specifically, to test if the overall testing error of several separate sessions would decrease when using case 2 rather than case 1.

\subsection{Case 3 and 4}
\label{sec:discussion-case3and4}
% In case 3, the model was trained on subjects 1 and 6, and tested on subject 2.
% The results indicate high level of overfitting with the error of the training set being much lower than for the validation set and the testing set.
The results from case 3 indicated that the model does not generalize well across subjects.
However, as seen in case 4, it depends on which subjects are being trained on and which are being tested on.

It is difficult to say why the model performs better in case 4 than case 3.
One explanation could be the discrepancy in the \ac{EMG} signal of the gluteus maximus, discussed in chapter \ref{sec:results}
When looking at figure \ref{fig:emg-average} it is clear that the \ac{EMG} signal from gluteus maximus of subject 2 varies quite a bit from subjects 1 and 6.
In that way, the pattern of this \ac{EMG} signal is completely unknown to the model in case 3.
Furthermore, since the overall amplitude of the signal is much lower in the training set, the model may be too sensitive to the high values from the test set.

\subsection{Case 5}
\label{sec:discussion-case5}
In case 5, the model was trained on all the subjects, where 80\% of the data was randomly selected for the training set and the rest for the testing set.
The model performed well in this case and the error for the validation set and testing set about the same.
This is as expected since they are both unseen data subsets of the same dataset.

\subsection{Comparison}

% The study included 5 subjects and two of those subjects measured minimal amount of non-corrupt data.
% Thus, only 3 subjects measured enough data to be used to train the \acp{NN}.
% From this small population it is difficult to draw any meaningful conclusion.

\section{Real-time compatability}
\label{sec:discussion_real-time}
When used in an exoskeleton, the assistance (or resistance) of the exoskeleton may affect the \ac{EMG} signal.
Since the model is based on the 20 past \ac{EMG} signals, this may create a bad feed-back loop.
For example, if the model uses \ac{EMG} signals from $t-20$ to $t-1$ to make the exoskeleton assist knee-flexion at $t$ then this could affect the \ac{EMG} signal collected at $t$.
This \ac{EMG} signal may be contradicting intended movement, and thus the prediction at $t+1$ might not be correct.
This in turn has more affect on the user's motion and their \ac{EMG} signal, thus resulting in the bad feed-back loop.

\end{document}